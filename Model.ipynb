{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-04 09:43:35.293077: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-06-04 09:43:35.293320: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/granthough/Documents/GitHub/ControversyScanner/Model.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/granthough/Documents/GitHub/ControversyScanner/Model.ipynb#ch0000000?line=16'>17</a>\u001b[0m dfNotToxicDownsampled \u001b[39m=\u001b[39m dfNotToxic\u001b[39m.\u001b[39msample(dfToxic\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39m#assume that the data is shuffled (it shouldn't be in order and it wouldn't matter anyway), now has 24153 values\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/granthough/Documents/GitHub/ControversyScanner/Model.ipynb#ch0000000?line=18'>19</a>\u001b[0m xTrain, xTest, yTrain, yTest \u001b[39m=\u001b[39m train_test_split(dfToxic, dfNotToxicDownsampled, test_size \u001b[39m=\u001b[39m \u001b[39m0.33\u001b[39m, random_state \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m) \u001b[39m#split data, stratify ensures distribution of categories is equal\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/granthough/Documents/GitHub/ControversyScanner/Model.ipynb#ch0000000?line=20'>21</a>\u001b[0m xTrain \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mconvert_to_tensor(xTrain, dtype \u001b[39m=\u001b[39;49m tf\u001b[39m.\u001b[39;49mstring)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/granthough/Documents/GitHub/ControversyScanner/Model.ipynb#ch0000000?line=22'>23</a>\u001b[0m bertPreprocess \u001b[39m=\u001b[39m hub\u001b[39m.\u001b[39mKerasLayer(\u001b[39m\"\u001b[39m\u001b[39mhttps://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/granthough/Documents/GitHub/ControversyScanner/Model.ipynb#ch0000000?line=23'>24</a>\u001b[0m bertEncoder \u001b[39m=\u001b[39m hub\u001b[39m.\u001b[39mKerasLayer(\u001b[39m\"\u001b[39m\u001b[39mhttps://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_text3.8/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/granthough/miniforge3/envs/tf_text3.8/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///Users/granthough/miniforge3/envs/tf_text3.8/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> <a href='file:///Users/granthough/miniforge3/envs/tf_text3.8/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=152'>153</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    <a href='file:///Users/granthough/miniforge3/envs/tf_text3.8/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=153'>154</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/granthough/miniforge3/envs/tf_text3.8/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py?line=154'>155</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/tf_text3.8/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/granthough/miniforge3/envs/tf_text3.8/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=99'>100</a>\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    <a href='file:///Users/granthough/miniforge3/envs/tf_text3.8/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=100'>101</a>\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> <a href='file:///Users/granthough/miniforge3/envs/tf_text3.8/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=101'>102</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np \n",
    "from keras import backend as K\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "df = pd.read_csv(\"FinalBalancedDataset.csv\") # 1 = toxic, 0 = not toxic\n",
    "df.groupby('Toxicity').describe() #32592 not toxic, 24153 toxic, imbalanced dataset so I need to downsample, only take 24153 not toxic samples so we have balanced data inputs\n",
    "\n",
    "dfToxic = df[df['Toxicity'] == 1] #originally had as '1', but it's an int not a character so you can just set to 1 \n",
    "dfNotToxic = df[df['Toxicity'] == 0]\n",
    "\n",
    "dfNotToxicDownsampled = dfNotToxic.sample(dfToxic.shape[0]) #assume that the data is shuffled (it shouldn't be in order and it wouldn't matter anyway), now has 24153 values\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(dfToxic, dfNotToxicDownsampled, test_size = 0.33, random_state = 42) #split data, stratify ensures distribution of categories is equal\n",
    "\n",
    "# xTrain = tf.convert_to_tensor(xTrain, dtype = tf.string)\n",
    "# yTrain = tf.convert_to_tensor(xTrain, dtype = tf.string)\n",
    "\n",
    "bertPreprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "bertEncoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
    "\n",
    "def getSentenceEmbedding(sentences): #basically turns sentences into tensors\n",
    "    \n",
    "    preprocessedText = bertPreprocess(sentences) #first you preprocess, then you encode\n",
    "    return bertEncoder(preprocessedText)['pooled_output'] #encoding basically turns data into numbers\n",
    "\n",
    "#defining layers\n",
    "textInput = tf.keras.layers.Input(shape = (), dtype = tf.string, name = \"text\")\n",
    "\n",
    "preprocessedText = bertPreprocess(textInput)\n",
    "outputs = bertEncoder(preprocessedText)\n",
    "\n",
    "layerOne = tf.keras.layers.Dropout(0.1, name = 'dropout')(outputs['pooled_output'])\n",
    "layerTwo = tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'ouptut')(layerOne)\n",
    "\n",
    "#define model\n",
    "model = tf.keras.Model(inputs = [textInput], outputs = [layerTwo])\n",
    "\n",
    "metrics = [\n",
    "    tf.keras.metrics.BinaryAccuracy(name = 'accuracy'),\n",
    "    tf.keras.metrics.Precision(name = 'precision'),\n",
    "    tf.keras.metrics.Recall(name = 'recall')\n",
    "]\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = metrics)\n",
    "\n",
    "model.fit(xTrain, yTrain, epochs = 10)\n",
    "\n",
    "#didn't work, need to pass in tokenized data, not string and then tokenize :(\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1d11444169456c54d28c4cdb3327aaab7a16f65683aa5ebd8651319d0a6beac3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf_text3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
